{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import numpy as np\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dyna_PCA.norm import _max_\n",
    "# from dyna_PCA.train_split import divide_shuffle,divide_shuffle_fem\n",
    "# from dyna_PCA.get_matrix import get_matrix\n",
    "# from dyna_PCA.get_pca import get_pc\n",
    "# from dyna_PCA.time_history import add_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = range(0,81)\n",
    "# ids= range(0,100)\n",
    "\n",
    "# ts = pd.DataFrame(list)\n",
    "# n_timesteps = len(ts)\n",
    "# nodes_ids = pd.DataFrame(ids)\n",
    "# sim = np.random.rand(24,81,3,104)\n",
    "# sim_index = range(0,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,valid,train_idx, valid_idx,= train_test_split(sim,sim_index, test_size=0.2, shuffle= True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix= get_matrix(train)\n",
    "# validMatrix= get_matrix(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008236702244509305 for n_comps: 244\n",
      "MAE: 0.07233889672077311 for n_comps: 244\n"
     ]
    }
   ],
   "source": [
    "# n_pca = 0.9\n",
    "# scaler = StandardScaler(with_mean=True, with_std=True) \n",
    "# scaler_std = scaler.fit(Matrix)\n",
    "# y_Data_std = scaler_std.transform(Matrix)\n",
    "\n",
    "# var = PCA().fit(y_Data_std).explained_variance_\n",
    "# cumulative_variance = np.cumsum(PCA().fit(y_Data_std).explained_variance_ratio_)\n",
    "\n",
    "# pca = PCA(n_components = n_pca)\n",
    "# y_PCA = pca.fit_transform(pd.DataFrame(y_Data_std))\n",
    "# n_comps = pca.n_components_\n",
    "\n",
    "# # MSE to for Pc error\n",
    "# y_PCA_inverse = pca.inverse_transform(y_PCA) # Inverse transfrom of sample data as per PCs to original form\n",
    "# y_PCA_InvTrans_Data = scaler_std.inverse_transform(y_PCA_inverse, copy=None) # inverse standardization\n",
    "# pca_mse = mean_squared_error(Matrix,y_PCA_InvTrans_Data)\n",
    "# pca_mae = mean_absolute_error(Matrix,y_PCA_InvTrans_Data)\n",
    "# print(\"MSE: {} for n_comps: {}\".format(pca_mse, n_comps))\n",
    "# print(\"MAE: {} for n_comps: {}\".format(pca_mae, n_comps))\n",
    "\n",
    "# ## Flattening PCA scores according to n_samples. For training\n",
    "# reshape_y_pca2 =[]\n",
    "# pd.DataFrame(reshape_y_pca2)\n",
    "# for i in range(0,len(train_idx)):\n",
    "#     temp_1 = y_PCA[(i*(n_timesteps-1)+i):(i*(n_timesteps-1)+i+n_timesteps)].T\n",
    "#     temp_2 = temp_1.flatten()\n",
    "#     temp = pd.DataFrame(temp_2)\n",
    "#     reshape_y_pca2.append(temp.T)\n",
    "\n",
    "# np.row_stack(reshape_y_pca2)   \n",
    "# y_PCA_ReSh = pd.DataFrame(np.row_stack(reshape_y_pca2), index = train_idx) \n",
    "# y_train =y_PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
